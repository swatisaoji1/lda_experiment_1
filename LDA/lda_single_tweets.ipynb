{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4><u>This experiment considers one tweet as one document.</u></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from functools32 import lru_cache\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing lemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatize = lru_cache(maxsize=50000)(wordnet_lemmatizer.lemmatize)\n",
    " \n",
    "# ===========helper methods ========================================\n",
    "\n",
    "\n",
    "def remove_non_ascii(s):\n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "\n",
    "def stop_words_list():\n",
    "    \"\"\"\n",
    "        A stop list specific to the observed timelines composed of noisy words\n",
    "        This list would change for different set of timelines\n",
    "    \"\"\"\n",
    "    stop_words = ['bc', 'http', 'https', 'co', 'com','rt', 'one', 'us', 'new',\n",
    "              'lol', 'may', 'get', 'want', 'like', 'love', 'no', 'thank', 'would', 'thanks',\n",
    "              'good', 'much', 'low', 'roger']\n",
    "\n",
    "    stoplist  = set( nltk.corpus.stopwords.words(\"english\") + stop_words)\n",
    "    return stoplist\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r\"(?:\\@|http?\\://)\\S+\", \"\", text)\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    helper function to readTweets() removes url and tokenizes text\n",
    "    :param text\n",
    "    \"\"\"\n",
    "    text = remove_urls(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = re.sub(r\"\"\"[\\'\\\"]\"\"\",'', text)\n",
    "    regexps = (\n",
    "        r\"\"\"(?:[\\w_]+)\"\"\",                          # regular word\n",
    "        r\"\"\"(?:[a-z][a-z'\\-_]+[a-z])\"\"\"             # word with an apostrophe or a dash\n",
    "    )\n",
    "    tokens_regexp = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regexps),\n",
    "                               re.VERBOSE | re.I | re.UNICODE)\n",
    "    return tokens_regexp.findall(text)\n",
    "\n",
    "\n",
    "def replace_acronym(tokens, slang_dict):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in slang_dict:\n",
    "            new_tokens.extend(slang_dict[token].split())\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(text, slang_dict, stop_words):\n",
    "    # get the tokens, lowercase - replace acronym\n",
    "    lowered = [item.lower() for item in tokenize(text)]\n",
    "    tokens = replace_acronym(lowered, slang_dict)\n",
    "\n",
    "    \n",
    "    tokens_pos = pos_tag(tokens)\n",
    "    words = []\n",
    "    for token in tokens_pos:\n",
    "        pos = get_wordnet_pos(token[1])\n",
    "        # if verb, noun, adj or adverb include them after lemmatization\n",
    "        if pos is not None and token[0] not in stop_words:\n",
    "            try:\n",
    "                tok = lemmatize(token[0], pos)\n",
    "                words.append(tok)\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "    # print words\n",
    "    return words\n",
    "\n",
    "def read_in_dict(filename):\n",
    "    dict = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.partition(\":\")\n",
    "            dict[parts[0].strip()] = parts[2].strip()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   user_id       username  \\\n0           0  17312100   FitYourStyle   \n1           1  17312100   FitYourStyle   \n2           2  17312100   FitYourStyle   \n3           3  17312100   FitYourStyle   \n4           4  17312100   FitYourStyle   \n\n                                      old_tweet_list  \n0  Yes! Love, love, love ! Enjoy the day @FitYour...  \n1  Hi, @FitYourStyle I hope u are fine. I just lo...  \n2                 . @LoriRMixson Hello from Toronto!  \n3  @FitYourStyle Thank you Jennifer - Hi from #Te...  \n4  Love from #Aurora #TheHip pic.twitter.com/4A2L...  \n"
     ]
    }
   ],
   "source": [
    "# read the tweet csv\n",
    "df = pd.read_csv('LDA/data/single_tweet_doc_AmericanCrime_1000.csv')\n",
    "print df[:5]"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 3,
   "source": [
    "We have read the tweets in dataframe : Now we take 2 paths \n",
    "<li>1. Use single tweet as One document</li>\n",
    "<li>2. Use all tweets from one user as One document</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Yes! Love, love, love ! Enjoy the day @FitYour...\n1    Hi, @FitYourStyle I hope u are fine. I just lo...\nName: old_tweet_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# get the tweets document\n",
    "list_of_single_documents = df.iloc[: , 3]\n",
    "print list_of_single_documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents\n",
    "def get_preprocessed_docs(list_of_doc):\n",
    "    stop_words = stop_words_list()\n",
    "    slang_dict = read_in_dict(\"LDA/data/out_slang_map.csv\")\n",
    "    for each_doc in list_of_doc:\n",
    "        yield tokenize_and_lemmatize(each_doc, slang_dict, stop_words)\n",
    "\n",
    "\n",
    "documents = get_preprocessed_docs(list_of_single_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yes', 'enjoy', 'day', 'proud', 'frenchcanadian', u'represent', 'boston', 'twitter', 'fityourstyle', u'tatu'], ['hi', 'hope', 'u', 'acronym', 'rich', 'environment', 'fine', 'see', u'book', u'author', 'reader', 'hopr', 'learn', 'lot', u'tweet'], ['hello', 'toronto'], ['jennifer', 'texas', 'kiss'], ['aurora', 'thehip', 'picture', 'twitter']]\n"
     ]
    }
   ],
   "source": [
    "list_doc = list(documents)\n",
    "print(list_doc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gensim dictionary from documents\n",
    "dictionary = corpora.Dictionary(list_doc)\n",
    "\n",
    "# filter extremes no_below=5, no_above=0.5, keep_n=100000\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.9)\n",
    "\n",
    "# save the dictionary\n",
    "dictionary.save('LDA/data/single_tweet_doc_AmericanCrime_1000_dict.dict')\n",
    "\n",
    "token_count = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(363, 1),\n  (2008, 1),\n  (6060, 1),\n  (6596, 1),\n  (7401, 1),\n  (8526, 1),\n  (9645, 1),\n  (10878, 1),\n  (11201, 1)],\n [(447, 1),\n  (1055, 1),\n  (1835, 1),\n  (2145, 1),\n  (2931, 1),\n  (4984, 1),\n  (6675, 1),\n  (7027, 1),\n  (8518, 1),\n  (10253, 1),\n  (11126, 1),\n  (11705, 1),\n  (13527, 1),\n  (13619, 1)],\n [(2503, 1), (3694, 1)],\n [(2732, 1), (7868, 1), (10938, 1)],\n [(6596, 1), (11087, 1)]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the dictionary to a corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in list_doc]\n",
    "\n",
    "# save corpus to the disck\n",
    "corpora.MmCorpus.serialize('LDA/data/single_tweet_doc_AmericanCrime_1000_corpus.mm', corpus) \n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}